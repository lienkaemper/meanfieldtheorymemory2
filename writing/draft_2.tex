\documentclass [12pt]{amsart}
\usepackage{amssymb,latexsym,amsfonts, amsthm, amsmath, amsrefs, mathtools,color}
\usepackage{array}
\usepackage{enumerate}
%\usepackage{enumitem}
\usepackage{booktabs}
\usepackage{fancyhdr}
\usepackage{verbatim}
\usepackage[margin=1in,twoside=false]{geometry}
\usepackage{multicol}
\usepackage{longtable}
\usepackage{hyperref}
\usepackage[capitalize]{cleveref}
\usepackage{mathrsfs}
\usepackage{tikz}
\usetikzlibrary{matrix,arrows}

\usepackage{soul}
\usepackage{blkarray}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%% Page layout and formatting
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%


\newcommand{\matr}[1]{\mathbf{#1}} % undergraduate algebra version
%\newcommand{\matr}[1]{#1}          % pure math version
%\newcommand{\matr}[1]{\bm{#1}}     % ISO complying version

%A_{k\ell}
\renewcommand{\vec}[1]{\mathbf{#1}} % undergraduate algebra version
%\renewcommand{\vec}[1]{#1}          % pure math version
%\renewcommand{\vec}[1]{\bm{#1}}     % ISO complying version


%\input{shortcuts}
\def\S{\mathbb{S}}
\def\cA{\mathcal{A}}
\def\cL{\mathcal{L}}
\def\cT{\mathcal{T}}
\def\cN{\mathcal{N}}



\def \R {\mathbb R}

%operators 
\DeclareMathOperator{\sign}{sign}
\DeclareMathOperator{\spa}{span}
\DeclareMathOperator{\conv}{conv}
\DeclareMathOperator{\Tr}{Tr}
\DeclareMathOperator{\var}{var}

%\DeclareMathOperator{\log}{log}

%vectors 
\def \vv{\mathbf v}
\def \w{\mathbf w}	
\def \s{\mathbf s}
\def \t{\mathbf t}


\newcommand\abk[1]{\left\langle #1 \right\rangle}


%Theorems
\theoremstyle{definition}
\newtheorem{thm}{Theorem}
\newtheorem{ithm}{Theorem}
\newtheorem{cor}{Corollary}

\newtheorem{lem}{Lemma}
\newtheorem{prop}{Proposition}
\newtheorem{defn}{Definition}
\newtheorem{ex}{Example}


\newcommand{\inv}{^{-1}}

\newcommand{\note}[1]{\textbf{\color{ForestGreen} #1}}


\title{Suppressing spontaneous correlations in memory models}


\begin{document}
\maketitle

\section{Model}
Let notation be as follows: 

\begin{itemize}
\item $r_i(t)$: the firing rate of neuron $i$ at time $t$. 
\item $\matr J$: matrix of synaptic filters. Entries are functions $\vec j_{ij}(t)= j_{ij} j(t) $, where $j(t)$ is the same filter for all synapses, $j_{ij}$ is the synaptic weight that neuron $i$ receives from neuron $j$. 
\item $N_i(t)$ the number of spikes neuron $i$ has fired up to time $(t)$. The spike train $\frac{dN_i}{dt}$ is a sum of delta functions centered on times that neuron $i$ spikes. The vector with all spike trains is written $\frac{d\vec N}{dt}$. 
\item $\phi$ nonlinear transfer function, from input rate to voltage. 
\item $v_i(t), \vec v(t)$. membrane voltage, in scalar and vector forms
\item $\omega$: dummy frequency variable for Fourier transform 
\item $h$: relative engram strength (excitatory)
\item $h_i$: relative engram strength (inhibitory)
\item $g$: strength of inhibition relative to excitation 
\item $g_{II}$: strength of CA1 inhibitory-to-inhibitory synapses
\item $\lambda_i$: external input - threshold to neural $i$
\item $J$: parameter scaling all synaptic weights 
\item $\matr A$: adjacency matrix 
\end{itemize}

With notation as above, we are considering the Poisson generalized linear model (GLM), also known as the Hawkes process, where the spike train $\frac{dN_i}{dt}$ is generated by a Poisson process with rate $r_i(t)$ given by
\begin{align}
r_i(t) = \phi\left(\sum_{j} \left(\vec j_{ij} * \frac{dN_j}{dt}\right)(t) + \lambda_i(t)\right)
\label{eqn:poisson_rate}
\end{align}

\subsection{hippocampal connectivity}

\begin{figure}
\includegraphics{hippocampus_cartoon_v2.pdf}
\caption{\label{fig:hippocampus}}
\end{figure}

In our model of memory formation, we consider two regions of the hippocampus, CA1 and CA3, illustrated in Figure \ref{fig:hippocampus}. Within both regions, neurons split into an excitatory and an inhibitory sub-population. In each region, we further subdivide the excitatory sub-population into \emph{engram cells} and \emph{non-engram cells}. The engram cells are those neurons which are involved in the memory trace formation, i.e. they are the population of neurons which are active at the time of fear memory conditioning, and which are re-activated by the conditioned stimulus in order to generate a fear response. We model memory formation as an increase in synaptic weights between engram cells, both within CA3 and in projections from CA3 to CA1. In CA1, the engram cells in the model correspond to the c-Fos tagged cells in the experiment, possibly with an allowance for noisy tagging. 
Connection probabilities between pairs of neurons by region are given by 
\begin{table}[ht!]
\begin{tabular}{lllll}
               & CA3 Excitatory & CA3 Inhibitory & CA1 Excitatory & CA1 Inhibitory \\ 
CA3 Excitatory & pEE            & pEI            & 0              & 0              \\
CA3 Inhibitory & pIE            & pII            & 0              & 0              \\
CA1 Excitatory & pEE            & 0              & 0              & pEI            \\
CA1 Inhibitory & pIE            & pII            & 0              & pII            \\     
\end{tabular}\\

We have sparse excitatory connectivity and dense excitatory connectivity, given by
\end{table}
\begin{align*}
pEE = 0.1\qquad
pIE = 0.1\qquad
pII = 0.8\qquad
pEI = 0.8.
\end{align*}
Notice that the connection probabilities do not differ between engram and non-engram cells. 
The adjacency matrix $\matr A$ of the network is given by 

\begin{align*}
\matr A_{ij} \sim \mathrm{Bernoulli}(p_{ij})
\end{align*}
The mean input a neuron in region $i$ receives from all neurons in region $j$ is given by the $i,j$ entry in the matrix $\overline{\matr J}$ below. 


\[
\overline{\matr J} = J \,\, \begin{blockarray}{ccccccc}
 CA3 E & CA3 P & CA3 I & CA1 E & CA1 P  & CA1 I \\
\begin{block}{(cccccc)c}
h & 1 & -g & 0 & 0 & 0 &CA3 E \\
 1 & 1 & -g & 0 & 0 & 0 &CA3 P\\
 1 & 1 & -g & 0 & 0 & 0 &CA3 I \\
h & 1 & 0 & 0 & 0 & -g &CA1 E    \\
1 & 1 & 0 & 0 & 0 & -g &CA1 P  \\
1 & 1& 0 & 1 & 1 & -g_{II}  &CA1 I  \\
\end{block}
\end{blockarray}
 \]
 


The full connectivity of the network is given by a random matrix $\matr J$, with 

\begin{align*}
\matr J_{ij} \sim \frac{\overline{\matr J_{ij}}}{N_j p_{ij}}\matr A_{ij}
\end{align*}
Notice that 
\begin{align*}
\abk{\sum_{k\in \mathrm{ region } j}J_{ik}}= \overline{J_{ij}}.
\end{align*}

Our first model of memory formation is as a strengthening of the relative weight of the engram to engram cell synapses, both within CA3 and from CA3 onto CA1. We denote this parameter describing the relative strength of the engram-to engram synapses as $h$.
We denote the relative weight of inhibitory synapses relative to excitatory synapses with the parameter $g$. We denote the relative weight of inhibitory-to-inhibitory synapses in region CA1 with the parameter $g_{II}$. Finally, we scale all synaptic weights by the parameter $J$. 
Our goal with this model is to observe how both the firing rates of each population and the correlations of pairs of neurons within and between each pair of populations change after learning, i.e. how they depend on the parameter $h$. 
We will describe these rates and correlations both in terms of the full weight matrix $\matr J$ and the regional weight matrix $\overline {\matr J}$. 

\subsection{estimate for the rates and covariances: threshold linear model}
We first work with a threshold-linear transfer function, $\phi(x) = \max(x,0)$. Further, we work under the assumption that all rates are positive, thus when computing the expected rates and the spike train covariances, we are able to fully drop $\phi$. In order to compute the expected rate in this linear model, we take expected values of both sides of the equation \ref{eqn:poisson_rate} and apply the linearity of the expectation

\begin{align*}
\abk{r_i} &= \abk{\sum_{j} \left(j_{ij} \frac{dN_j}{dt}\right) + \lambda_i}\\
\abk{r_i} &= \sum_{j} \left(j_{ij} \abk{\frac{dN_j}{dt}}\right) + \lambda_i\\
\abk{r_i} &= \sum_{j} \left(j_{ij} \abk{r_i}\right) + \lambda_i\\
\end{align*}
We then solve the resulting matrix equation for $\abk{\vec r}$, obtaining 
\begin{align}
\abk{\vec r} = (\matr I - \matr J)\inv \vec \lambda  
\label{eqn:expected_rate}
\end{align}
Now, we seek an expectation value for the mean population rate $\bar{\vec r}$, where $\bar{r_i} = \frac{1}{N_i} \sum_{k\in\, \mathrm{ region }\, i} r_k$. We will take this expectation over both the frozen disorder, i.e. the weight matrix $\overline {\vec J}$, and the stochastic generation of spike trains. We have

\begin{align*}
\abk{\overline{r_i}}_{\matr J, \vec N} = \frac{1}{N_i}\abk{(\matr I - \matr J)\inv \vec \lambda }_{\matr J}. 
\end{align*}

Now, we use the expansion $(\matr I - \matr J)\inv = \sum_{n = 0}^\infty \matr {J}^n$, which is valid when all eigenvalues of $\matr J$ have real part less than 1. This is the same condition we require for the fixed point to be stable. Thus, we need to give an expression for $\abk{(\matr J^n)_{ij}}_{\matr J}$ in terms of $\overline {\matr J}$, $p$, and $N$. 
We have 
\begin{align*}
(\matr J^n)_{ij} = \sum_{i = \ell_0, \ldots, \ell_n = j} \prod_{k = 0}^{n-1} \matr J_{\ell_k \ell_{k+1}} 
= \sum_{i = \ell_0, \ldots, \ell_n = j} \prod_{k = 0}^{n-1} \frac{\overline{\matr J_{\ell_k \ell_{k+1}}}}{N_{\ell_{k+1}} p_{\ell_{k}\ell_{k+1}}}A_{\ell_{k}\ell_{k+1}},
\end{align*}
where here, $\overline{\matr J}_{\ell_k \ell_{k+1}}$, $ p_{\ell_{k}\ell_{k+1}}$, and $N_{\ell_{k+1}}$ refer to the values of $\overline{\matr J}$, $p$, and $N$ for the populations $\ell_k$ and  $\ell_{k+1}$.

Now, we separate the deterministic and random parts of this expression, in order to give an expression for  $\abk{(\matr J^n)_{ij}}_{\matr J}$:

\begin{align*}
\abk{(\matr J^n)_{ij}}_{\matr J} =  \sum_{i = \ell_0, \ldots, \ell_n = j} \prod_{k = 0}^{n-1} \frac{\overline{\matr J_{\ell_k \ell_{k+1}}}}{N_{\ell_{k+1}}  p_{\ell_{k}\ell_{k+1}}}\prod_{k = 0}^{n-1}\abk{A_{\ell_{k}\ell_{k+1}}},
\end{align*}





\subsection{one loop correction to rates, threshold quadratic model}



\bibliography{memory}
\bibliographystyle{plain}
\end{document}

