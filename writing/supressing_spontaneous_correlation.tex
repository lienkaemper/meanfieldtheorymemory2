\documentclass [12pt]{amsart}
\usepackage{amssymb,latexsym,amsfonts, amsthm, amsmath, amsrefs, mathtools,color}
\usepackage{array}
\usepackage{enumerate}
%\usepackage{enumitem}
\usepackage{booktabs}
\usepackage{fancyhdr}
\usepackage{verbatim}
\usepackage[margin=1in,twoside=false]{geometry}
\usepackage{multicol}
\usepackage{longtable}
\usepackage{hyperref}
\usepackage[capitalize]{cleveref}
\usepackage{mathrsfs}
\usepackage{tikz}
\usetikzlibrary{matrix,arrows}

\usepackage{soul}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%% Page layout and formatting
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%% Notation shortcuts
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

%\input{shortcuts}
\def\S{\mathbb{S}}
\def\cA{\mathcal{A}}
\def\cL{\mathcal{L}}
\def\cT{\mathcal{T}}
\def\cN{\mathcal{N}}



\def \R {\mathbb R}

%operators 
\DeclareMathOperator{\sign}{sign}
\DeclareMathOperator{\spa}{span}
\DeclareMathOperator{\conv}{conv}
\DeclareMathOperator{\Tr}{Tr}


\newcommand{\matr}[1]{\mathbf{#1}} % undergraduate algebra version
%\newcommand{\matr}[1]{#1}          % pure math version
%\newcommand{\matr}[1]{\bm{#1}}     % ISO complying version

%A_{k\ell}
\renewcommand{\vec}[1]{\mathbf{#1}} % undergraduate algebra version
%\renewcommand{\vec}[1]{#1}          % pure math version
%\renewcommand{\vec}[1]{\bm{#1}}    

%vectors 
\def \vv{\mathbf v}
\def \w{\mathbf w}	
\def \s{\mathbf s}
\def \t{\mathbf t}

\renewcommand{\vec}{\mathbf}	

\newcommand\abk[1]{\left\langle #1 \right\rangle}


%Theorems
\theoremstyle{definition}
\newtheorem{thm}{Theorem}
\newtheorem{ithm}{Theorem}
\newtheorem{cor}{Corollary}

\newtheorem{lem}{Lemma}
\newtheorem{prop}{Proposition}
\newtheorem{defn}{Definition}
\newtheorem{ex}{Example}


\newcommand{\inv}{^{-1}}

\usepackage{subcaption}
\usepackage{blkarray}


\title{Suppressing spontaneous correlations in memory models}


\begin{document}
\maketitle

\section{Model}
\subsection{Poisson spiking model}
We model neural activity using a nonlinear spiking model taken from  \cite{ocker2017linking}. 
Let $N_i(t)$ denote the number of spikes fired by neuron $i$ up to time $t$. The spike train of the neuron is then a sum of delta functions $\frac{dN_i}{dt} = \sum_k \delta(t - t_i^k)$. Spikes are generated by a Poisson process with rate $r_i(t)$ given by
\begin{align}
r_i(t) = \phi\left(\sum_{j} \left(\mathbf g_{ij} * \frac{dN_j}{dt}\right)(t) + \lambda_i(t)\right)
\label{eqn:poisson_rate}
\end{align}
The filter $\mathbf g_{ij}$ is the product $g_{ij}(t) = \mathbf g_{ij}g(t)$ of the synaptic weight $g_{ij}$ from $j$ onto $i$  and a temporal filter $g(t) = \frac 1 \tau e^{-t/\tau}$. In our basic model, we will take $\phi$ as a threshold-linear function $\phi(x) = \max\{x,0\}$. We will also consider other activation functions. 

We can translate this to a differential equation for membrane voltage where the filter doesn't appear:

\begin{align*}
\tau \frac{dv_i}{dt} = - v_i + \sum_{j} g_{ij} \frac{dN_j}{dt} + \tau\lambda_i(t)\\
r_i(t) = \phi(v_i(t))
\end{align*}

We can use this to write the joint density functional for $\vec v$ and $\vec N$. 
\small
\begin{align*}
p(v, N| \eta, J) &= \prod_{i = 1}^N \prod_{t= 1}^{T-1} \delta(\tau \frac{\Delta v_i}{\Delta t} + v_i - \sum_{j=1}^N J_{ij} \Delta N_j- \tau \lambda_i) \delta(\Delta N- \eta_{it})\\
p(v, N |\eta, J) &= \int D \vec{\tilde{v}} D\vec{\tilde{N}}\exp\left(-i\left [\sum_{i = 1}^N \sum_{t= 1}^{T-1}\tilde{v_i} (\tau \frac{\Delta v_i}{\Delta t} + v_i - \sum_{j=1}^N J_{ij} \Delta N_j- \tau \lambda_i) + \tilde{n_i} (\Delta N_{it} - \eta_{it}) \right]\right)\\
p(v, N|J) &= \int D \vec{\tilde{v}} D\vec{\tilde{N}}\exp\left(-i\left [\sum_{i = 1}^N \sum_{t= 1}^{T-1}\tilde{v_i} (\tau \frac{\Delta v_i}{\Delta t} + v_i - \sum_{j=1}^N J_{ij} \Delta N_j- \tau \lambda_i) + \tilde{n_i} \Delta N_{it}	- \log(f(v_{it})dt(e^{i \tilde{n_{it}}} -1) +1) \right]\right)
\end{align*}
Now, we take the limit as $\Delta t\to 0$, using the Taylor approximation $\log(1 + x) = x$. 

\small
\begin{align*}
p(v, N|J) = \int D \vec{\tilde{v}} D\vec{\tilde{N}}\exp\left(-i\left [\sum_{i = 1}^N \int_0^{T}dt \tilde{v_i} (\tau \frac{ dv_i}{dt} + v_i - \sum_{j=1}^N J_{ij}dN_j- \tau \lambda_i) + \tilde{n_i} d N_{it}	-f(v_{it})dt(e^{i \tilde{n_{it}}} -1) \right]\right)
\end{align*}

Now, we average this over the connectivity. 
\small
\begin{align*}
p(v, N) =&1 \int d\matr J P(\matr J) p(v, N | J)\\
p(v, N) =& \int  D \vec{\tilde{v}} D\vec{\tilde{N}}\exp\left(-i\left [\sum_{i = 1}^N \int_0^{T}dt \tilde{v_i} (\tau \frac{ dv_i}{dt} + v_i - \sum_{j=1}^N J_{ij}dN_j- \tau \lambda_i) + \tilde{n_i} d N_{it}	-f(v_{it})dt(e^{i \tilde{n_{it}}} -1) \right]\right)P(J)d(\matr J)\\
p(v, N) =& \int  D \vec{\tilde{v}} D\vec{\tilde{N}}\exp\left(-i\left [\sum_{i = 1}^N \int_0^{T}dt \tilde{v_i} (\tau \frac{ dv_i}{dt} + v_i - \tau \lambda_i) + \tilde{n_i} d N_{it}	-f(v_{it})dt(e^{i \tilde{n_{it}}} -1) \right]\right)\\&\int \exp(- \sum_{j=1}^N J_{ij}dN_j)P(J)d(\matr J)\\
\end{align*}
Now, we  not that $\int \exp(- \sum_{j=1}^N \int_0^T J_{ij}dN_j)P(J)d(\matr J)$ is the moment generating functional of $\matr J$, evaluated at  $ \int_0^T dN_j$. 
Thus, letting $W_J$ denote the cumulant generating functional for $J$, we have 
\begin{align*}
p(v, N) =& \int  D \vec{\tilde{v}} D\vec{\tilde{N}}\exp\left(-i\left [\sum_{i = 1}^N \int_0^{T}dt \tilde{v_i} (\tau \frac{ dv_i}{dt} + v_i - \tau \lambda_i) + \tilde{n_i} d N_{it}	-f(v_{it})dt(e^{i \tilde{n_{it}}} -1) + \sum_{j=1}^N W_{J}(\int \tilde{v_i} dN_j) \right]\right)\
\end{align*}
Now, by the definition and properties of the cumulant generating function, we have 
\begin{align*}
W_{J}(\int \tilde{v_i} dN_j) = \log\left (\int d J_{ij} p(J_{ij})\exp(-(\int \tilde{v_i} dN_j)\right) \\
W_{J}(\int \tilde{v_i} dN_j) = \sum_{n=1}^\infty \frac{\ll J_{ij}^n\gg }{n!}(\int \tilde{v_i} dN_j)^n 
\end{align*}
Now, the cumulants of $\ll J_{ij}^n\gg$ are easy to find, since these are just the cumulants of the Bernoulli distribution. These are 
\begin{align*}
\ll J_{ij}\gg = \frac{p \bar J}{N}\\
\ll J_{ij}^2\gg = (\frac{\bar J}{N})^2(p- p^2)\\
\end{align*}
So our justification for truncating is going to be that the $n$-th cumular scales as $N^{-n}$. The problem is that I don't think we can truncate at the first cumulant, even if we are looking to do things at order $\frac 1 N$, because the sum over $N$ will take $O(1/N^2)$ to $O(1/N)$. And since the correlations are $1/N$, we don't want to be sending this to zero. Although, this may be a real problem, not a made-up problem in our self-justification for what we're doing-- it seems like we are genuinely off in estimates of correlation, and this could be why. 



We compute the expected value of the firing rate for each neuron and the covariance between the spike trains of each pair of neurons in terms of the matrix of filters $\mathbf G(t)$ with entries $\mathbf g_{ij}(t)$. 
First, we find the expected values of the firing rate, $r_i^* = \abk{\frac{d N_i}{dt} }$ by taking expected values of both sides of \ref{eqn:poisson_rate}, assuming that $\sum_j \mathbf g_{ij} * r^*_j > 0$ and thus dropping the threshold nonlinearity $\phi$. 
\begin{align*}
r_i^* &= \sum_j \left(\mathbf g_{ij} * r^*_j\right) + \lambda_i\\
r_i^* &= \sum_j W_{ij} r^*_j + \lambda_i\\
r^* &= W r^* + \mathbf \lambda \\
r^*  &= (I - W)\inv \mathbf \lambda
\end{align*}
The correlations are given by 
\begin{align*}
C = \int_{-\infty}^\infty \abk{\frac{dN}{dt}(t) \frac{dN}{dt}(t-\tau)^T}d\tau =  (I - W)\inv Y(I - W^T)\inv
\end{align*}


Floating citations: 
\cite{mendez2018homeostatic}

\subsection{Hippocampus and memory formation model}


\begin{figure}
\includegraphics{hippocampus_cartoon_v2.pdf}
\caption{\label{fig:hippocampus}}
\end{figure}

In our model of memory formation, we consider two regions of the hippocampus, CA1 and CA3, illustrated in Figure \ref{fig:hippocampus}. Within both regions, neurons split into an exictatory and an inhibitory sub-population. In each region, we futher subdivide the excitatory sub-population into a \emph{engram cells} and \emph{non-engram cells}. The engram cells are those neurons which are involved in the memory trace formation, i.e. they are the population of neurons which are active at the time of fear memory conditioning, and which are re-activated by the conditioned stimulus in order to generate a fear response. In CA1, the engram cells in the model correspond to the c-Fos tagged cells in the experiment, possibly with an allowance for noisy tagging. 
Connection probabilities between pairs of neurons by region are given by 
\begin{table}[]
\begin{tabular}{lllll}
               & CA3 Excitatory & CA3 Inhibitory & CA1 Excitatory & CA1 Inhibitory \\ 
CA3 Excitatory & pEE            & pEI            & 0              & 0              \\
CA3 Inhibitory & pIE            & pII            & 0              & 0              \\
CA1 Excitatory & pEE            & 0              & 0              & pEI            \\
CA1 Inhibitory & pIE            & pII            & 0              & pII            \\     
\end{tabular}\\
We have sparse excitatory connectivity and dense excitatory connectivity, given by
\end{table}
\begin{align*}
pEE = 0.1\qquad
pIE = 0.1\qquad
pII = 0.8\qquad
pEI = 0.8.
\end{align*}
Notice that the connection probabilities do not differ between engram and non-engram cells. 
The mean input a neuron in each region receives from each region is  given in the matrix $\overline G$ below. 


{\color{purple} Need to scale this by the number of connections}

\[
G= J \,\, \begin{blockarray}{ccccccc}
 CA3 E & CA3 P & CA3 I & CA1 E & CA1 P  & CA1 I \\
\begin{block}{(cccccc)c}
h & 1 & -g & 0 & 0 & 0 &CA3 E \\
 1 & 1 & -g & 0 & 0 & 0 &CA3 P\\
 1 & 1 & -g & 0 & 0 & 0 &CA3 I \\
h & 1 & 0 & 0 & 0 & -g &CA1 E    \\
1 & 1 & 0 & 0 & 0 & -g &CA1 P  \\
1 & 1& 0 & 1 & 1 & -g  &CA1 I  \\
\end{block}
\end{blockarray}
 \]
we

Our first model of memory formation is as a strengthening of the relative weight of the engram to engram cell synapses, both within CA3 and from CA3 onto CA1. We denote this parameter describing the relative strength of the engram-to engram synapses as $h$.
Our goal with this model is to observe how both the firing rates of each population and the correlations of pairs of neurons within and between each pair of populations change after learning, i.e. how they depend on the parameter $h$. 
We will describe these rates and correlations both in terms of the full connectivity matrix $\mathbf W$, as well as of a reduced $6\times 6$ connectivity matrix describing the mean input each neuron in one region receives from each other region. 


\subsection{Rates and covariances, linear version}
The propagator $(I-G)\inv$ for our memory model is 

\begin{align*}
\Delta = (I - G)\inv = \left(
\begin{array}{cccccc}
 \frac{-g j+j-1}{(g-1) (h-1) j^2+j (-g+h+1)-1} & -\frac{j}{(g-1) (h-1) j^2+j (-g+h+1)-1} & \frac{g j}{(g-1) (h-1) j^2+j (-g+h+1)-1} & 0 & 0 & 0 \\
 -\frac{j}{(g-1) (h-1) j^2+j (-g+h+1)-1} & \frac{g j ((h-1) j-1)+h j-1}{(g-1) (h-1) j^2+j (-g+h+1)-1} & \frac{g j (-h j+j+1)}{(g-1) (h-1) j^2+j (-g+h+1)-1} & 0 & 0 &
   0 \\
 -\frac{j}{(g-1) (h-1) j^2+j (-g+h+1)-1} & \frac{j ((h-1) j-1)}{(g-1) (h-1) j^2+j (-g+h+1)-1} & \frac{-h j^2+h j+j^2+j-1}{(g-1) (h-1) j^2+j (-g+h+1)-1} & 0 & 0 & 0
   \\
 \frac{j \left(j \left(g^2 j (j+1)-g j^2+g-1\right)-h \left(g^2 j^2+(g-1) g j^3+(2 g-1) j+1\right)\right)}{(g j (2 j+1)+1) \left((g-1) (h-1) j^2+j (-g+h+1)-1\right)}
   & -\frac{j \left(g \left((h-1) j^3+j\right)+1\right)}{(g j (2 j+1)+1) \left((g-1) (h-1) j^2+j (-g+h+1)-1\right)} & \frac{g j^2 \left(g h j^2+(g-1) h j-(j+1) (g
   j-1)+h\right)}{(g j (2 j+1)+1) \left((g-1) (h-1) j^2+j (-g+h+1)-1\right)} & \frac{g j (j+1)+1}{g j (2 j+1)+1} & -\frac{g j^2}{g j (2 j+1)+1} & -\frac{g j}{g j (2
   j+1)+1} \\
 \frac{j \left(g^2 (h-1) j^3-g j \left((h-1) j^2-h j+j+1\right)-1\right)}{(g j (2 j+1)+1) \left((g-1) (h-1) j^2+j (-g+h+1)-1\right)} & \frac{j \left(g (h-1) j^3+g
   (h-1) j^2+j (-g+h-1)-1\right)}{(g j (2 j+1)+1) \left((g-1) (h-1) j^2+j (-g+h+1)-1\right)} & \frac{g j^2 \left(j^2 (g-g h)-h j+j+2\right)}{(g j (2 j+1)+1)
   \left((g-1) (h-1) j^2+j (-g+h+1)-1\right)} & -\frac{g j^2}{g j (2 j+1)+1} & \frac{g j (j+1)+1}{g j (2 j+1)+1} & -\frac{g j}{g j (2 j+1)+1} \\
 -\frac{j \left(j^2 (g h+g-h+1)+j (g+h+1)+1\right)}{(g j (2 j+1)+1) \left((g-1) (h-1) j^2+j (-g+h+1)-1\right)} & \frac{j \left(2 g (h-1) j^3+j^2 (g (h-3)+h-1)+j
   (-g+h-3)-1\right)}{(g j (2 j+1)+1) \left((g-1) (h-1) j^2+j (-g+h+1)-1\right)} & \frac{2 g j^2 \left(-\left((h-1) j^2\right)+2 j+1\right)}{(g j (2 j+1)+1)
   \left((g-1) (h-1) j^2+j (-g+h+1)-1\right)} & \frac{j}{g j (2 j+1)+1} & \frac{j}{g j (2 j+1)+1} & \frac{1}{g j (2 j+1)+1} \\
\end{array}
\right)
\end{align*}
Specifically, $\Delta_{ij}$ gives the response of a neuron in population $i$ to changing the input to population $j$. 
So, in our memory model, $\Delta_{11}$ gives the response of CA3 Engram cells to stimulation of CA3 engram cells, and $\Delta_{41}$ gives the response of the CA1 Engram cells to stimulating the CA3 engram cells. 

These entries are as follows: 

\begin{align*}
\Delta_{11} &= \frac{-g j+j-1}{(g-1) (h-1) j^2+j (-g+h+1)-1} \\
\Delta_{41} &=  \frac{-h j + (-1 + g + (1 - 2 g) h) j^2 + (g^2 - g^2 h) j^3 + (-g + 
    g^2 + (1 - g) g h) j^4}{(g j (2 j+1)+1) \left((g-1) (h-1) j^2+j (-g+h+1)-1\right)}
\end{align*}

Term that's in all the CA3-CA3 denominators: 
$d_3 = -1 + j (1 - g + h + (-1 + g) (-1 + h) j)$.

Term that's in all the CA1-CA1 denominators: 
$d_1 = 1 + g j (1 + 2 j)$.

Rewriting the matrix making this substitution 

\begin{align*}
\Delta = (I - G)\inv = \left(
\begin{array}{cccccc}
 \frac{-g j+j-1}{d_3} & -\frac{j}{d_3} & \frac{g j}{d_3} & 0 & 0 & 0 \\
 -\frac{j}{d_3} & \frac{g j ((h-1) j-1)+h j-1}{d_3} & \frac{g j (-h j+j+1)}{d_3} & 0 & 0 &
   0 \\
 -\frac{j}{d3} & \frac{j ((h-1) j-1)}{d_3} & \frac{-h j^2+h j+j^2+j-1}{d_3} & 0 & 0 & 0
   \\
 \frac{j \left(j \left(g^2 j (j+1)-g j^2+g-1\right)-h \left(g^2 j^2+(g-1) g j^3+(2 g-1) j+1\right)\right)}{d_1d_3}
   & -\frac{j \left(g \left((h-1) j^3+j\right)+1\right)}{d_1d_3} & \frac{g j^2 \left(g h j^2+(g-1) h j-(j+1) (g
   j-1)+h\right)}{d_1d_3} & \frac{g j (j+1)+1}{d_3} & -\frac{g j^2}{d_3} & -\frac{g j}{d_3} \\
 \frac{j \left(g^2 (h-1) j^3-g j \left((h-1) j^2-h j+j+1\right)-1\right)}{d_1d_3} & \frac{j \left(g (h-1) j^3+g
   (h-1) j^2+j (-g+h-1)-1\right)}{d_1d_3} & \frac{g j^2 \left(j^2 (g-g h)-h j+j+2\right)}{d_1d_3} & -\frac{g j^2}{d_3} & \frac{g j (j+1)+1}{d_3} & -\frac{g j}{d_3} \\
 -\frac{j \left(j^2 (g h+g-h+1)+j (g+h+1)+1\right)}{d_1d_3} & \frac{j \left(2 g (h-1) j^3+j^2 (g (h-3)+h-1)+j
   (-g+h-3)-1\right)}{d_1d_3} & \frac{2 g j^2 \left(-\left((h-1) j^2\right)+2 j+1\right)}{d_1d_3} & \frac{j}{d_3} & \frac{j}{d_3} & \frac{1}{d_3} \\
\end{array}
\right)
\end{align*}


\begin{align*}
y = 
\begin{pmatrix}
-\frac{y}{(g-1) (h-1) j^2+j (-g+h+1)-1}\\
\frac{y ((h-1) j-1)}{(g-1) (h-1) j^2+j (-g+h+1)-1}\\
\frac{y ((h-1) j-1)}{(g-1) (h-1) j^2+j (-g+h+1)-1}\\
-\frac{y \left(g \left((h-1) j^3+j\right)+1\right)}{(g j (2 j+1)+1) \left((g-1) (h-1) j^2+j (-g+h+1)-1\right)}\\
\frac{y \left(g (h-1) j^3+g (h-1) j^2+j (-g+h-1)-1\right)}{(g j (2 j+1)+1) \left((g-1) (h-1) j^2+j (-g+h+1)-1\right)}\\
\frac{y \left(2 g (h-1) j^3+j^2 (g (h-3)+h-1)+j (-g+h-3)-1\right)}{(g j(2 j+1)+1) \left((g-1) (h-1) j^2+j(-g+h+1)-1\right)}
\end{pmatrix}
\end{align*}
  
The 

\cite{pernice2011structure}

\subsection{1-loop correction to rate in CA3}

Now, we're considering nonlinear activation $\phi$. For instance, threshold quadratic $\phi(x) = \max\{0, x^2\}$. Now, in addition to finding the mean-field rate which satisfies $\abk{\frac{dN}{dt}} = \phi(G \abk{ \frac{dN}{dt}} + \lambda_i)$, we need to account for the account for fluctuations. 

We do this by evaluating the following 

\begin{align*}
\Delta_{i,j}'(\omega)  = \sum_{k,l} \int d\omega' d\omega '' d \omega''' \Delta_{ik}^0(\omega)\frac{\phi_k''}{2}J_{k l}(\omega')\Delta^0_{kl}(w') J_{kl}(\omega'') \Delta^0_{kl}(\omega'') \delta(\omega-(\omega' + \omega''))\\\phi'_l J_{lj}(\omega''') \Delta_{lj}^0(\omega''')\delta(\omega' + \omega'' - \omega''')
\end{align*}
We evaluate this by first making substitutions based on the $\delta$-functions, then by putting in the actual values for $J$, $\phi$, and $\Delta^0$. 
\begin{align*}
\Delta_{i,j}'(\omega)  = \sum_{k,l} \int d\omega' d\omega ''  \Delta_{ik}^0(\omega)\frac{\phi_k''}{2}J_{k l}(\omega')\Delta^0_{kl}(w') J_{kl}(\omega'') \Delta^0_{kl}(\omega'') \delta(\omega-(\omega' + \omega''))\\\phi'_l J_{lj}(\omega' + \omega'') \Delta_{lj}^0(\omega' + \omega'')\\
\Delta_{i,j}'(\omega)  = \sum_{k,l} \int d\omega'   \Delta_{ik}^0(\omega)\frac{\phi_k''}{2}J_{k l}(\omega')\Delta^0_{kl}(w') J_{kl}(\omega - \omega') \Delta^0_{kl}(\omega - \omega') \\\phi'_l J_{lj}(\omega' + \omega - \omega') \Delta_{lj}^0(\omega' + \omega - \omega')\\
\Delta_{i,j}'(\omega)  = \sum_{k,l} \int d\omega'   \Delta_{ik}^0(\omega)\frac{\phi_k''}{2}J_{k l}(\omega')\Delta^0_{kl}(w') J_{kl}(\omega - \omega') \Delta^0_{kl}(\omega - \omega')\phi'_l J_{lj}(\omega ) \Delta_{lj}^0(\omega)\\
\end{align*}

This comes from path integral 

\begin{align*}
S = \int dt \sum_{i} \tilde n_i(t) n_i(t) - (e^{\tilde n_i} -1)\phi\left(\sum_j J_ij * \dot n_j)(t)\right)\\
P[n, \tilde n] = \exp(S)
\end{align*}
(Second part is me filling in, but i think it's right)

Stuff we need to calculate, for this: 

\begin{itemize}
\item  $G_{ij}(\omega)$
\item  $\Delta_{ij}^0(\omega)$
\item  $\phi'$, $\phi''$
\end{itemize}

First, we calculate 
\begin{align*}
G_{ij}(\omega) &= \int_{-\infty}^\infty g_{ij}g(t) e^{-2\pi i \omega t} dt \\
G_{ij}(\omega) &= \frac{g_{ij}}{\tau}\int_{0}^\infty e^{-t/\tau}e^{-2\pi i \omega t} dt \\
G_{ij}(\omega) &= \frac{g_{ij}}{\tau}\int_{0}^\infty e^{-t(1/\tau +2\pi i \omega) } dt \\
G_{ij}(\omega) &=   \frac{g_{ij}}{\tau} \frac{1}{\frac 1 \tau + 2\pi i \omega} =  \frac{g_{ij}}{1+ 2\pi i \omega \tau} 
\end{align*}
Note this has the desired property that $G_{ij}(0) = g_{ij}$, i.e. this works out to be the integrated filter. 

Now, goal is to find $\Delta_{ij}^0(\omega)$.  First, we find the propagator in the time domain $\Delta_{ij}^0(t, t')$, which measures the expected change in the rate  of neuron $i$ at time $t$ $r_i(t)$ in response to the addition of one spike to the input of neuron $j$ at time $t'$ in the linear model. 

\begin{align*}
\bar r_i(t)  + (\Delta_{ij} * \delta)(t, t') = \epsilon \sum_{j} \left(g_{ij}* \bar r_j (t) + g_{ij}*\Delta_{ij} *\delta(t,t') \right) + \lambda_i(t) + \delta_{ij}\delta(t-t')
\end{align*}

\begin{align*}
\Delta_{ij}(t,t') = \phi_i'\left(\sum_k (g_{ik} * \Delta_{kj})(t, t') +\delta_{ij} \delta(t -t')\right)
\end{align*}

Ok, so the way we calculate this is taking the Fourier transform of both sides, to turn the convolution into pointwise multiplication. 

\begin{align*}
\hat \Delta_{ij}(\omega) = \phi_i' \sum_{k}\left( \hat g_{ik} (\omega) \hat \Delta_{kj} (\omega)\right) + \delta_{ij}
\end{align*}

Now, writing in matrix form, 
\begin{align*}
\hat \Delta(\omega) = \phi' \hat G (\omega) \hat \Delta (\omega)+ I\\
\hat \Delta(\omega) - \phi' \hat G (\omega) \hat \Delta (\omega) = I\\
\hat \Delta(\omega)(I -   \phi' \hat G (\omega))= I\\
\hat \Delta(\omega)= (I -   \phi' \hat G (\omega))\inv\\
\end{align*}
Here, $\phi'$ is a diagonal matrix, entries are $\phi_i'$ evaluated at the mean field value. 

Ok, so now we need to take the fourier transform of our matrix of filters $G$. Luckily, we already did this
\begin{align*}
G_{ij}(\omega) &=   \frac{g_{ij}}{\tau} \frac{1}{\frac 1 \tau + 2\pi i \omega} =  \frac{g_{ij}}{1+ 2\pi i \omega \tau} 
\end{align*}

Now, we write out 

\begin{align*}
\Delta'_{ij}(\omega) = \sum_{k, l}\int d \omega' \left(I  - \frac{1}{1 + 2\pi i \omega\tau} \phi' G\right)_{ik}\inv \phi''_k \frac{G_{kl}}{1 + 2\pi i \omega' \tau} \left(I  - \frac{1}{1 + 2\pi i \omega'\tau} \phi' G\right)_{kl}\inv\\ 
\frac{G_{kl}}{1 + 2\pi i (\omega - \omega') \tau} \left(I  - \frac{1}{1 + 2\pi i (\omega - \omega')\tau} \phi' G\right)_{ik}\inv \phi'_l  \frac{G_{lj}}{1 + 2\pi i \omega \tau} \left(I  - \frac{1}{1 + 2\pi i (\omega - \omega')\tau} \phi' G\right)_{lj}\inv\\
%
\Delta'_{ij}(\omega) = \sum_{k, l} \left(I  - \frac{1}{1 + 2\pi i \omega\tau} \phi' G\right)_{ik}\inv \phi''_k  \phi'_l \frac{G_{lj}}{1 + 2\pi i \omega \tau}\int d \omega'\frac{G_{kl}}{1 + 2\pi i \omega' \tau} \left(I  - \frac{1}{1 + 2\pi i \omega'\tau} \phi' G\right)_{kl}\inv\\ 
\frac{G_{kl}}{1 + 2\pi i (\omega - \omega') \tau} \left(I  - \frac{1}{1 + 2\pi i (\omega - \omega')\tau} \phi' G\right)_{ik}\inv  \left(I  - \frac{1}{1 + 2\pi i (\omega - \omega')\tau} \phi' G\right)_{lj}\inv\\
\end{align*}

Limiting to CA3 only, we have 

\begin{align*}
\Delta_0(\omega) = \frac{1}{j (g-h-1) (2 \pi  i \omega +1)+j^2 (g (-h)+g+h-1)+(2 \pi  i \omega +1)^2}\\
\left(
\begin{array}{ccc}
 (2 \pi  i \omega +1) ((g-1) j+2 \pi  i \omega +1) & 2 \pi  i j \omega +j & -g j (2 \pi  i \omega +1) \\
 2 \pi  i j \omega +j & g j (-h j+2 \pi  i \omega +j+1)-(2 \pi  i \omega +1) (h j-2 \pi  i \omega -1) & g j ((h-1) j-2 \pi  i \omega -1) \\
 2 \pi  i j \omega +j & j (-h j+2 \pi  i \omega +j+1) & -(h+1) j (2 \pi  i \omega +1)+(h-1) j^2+(2 \pi  i \omega +1)^2 \\
\end{array}
\right)
\end{align*}

It probably makes sense to do this term by term, using the expansion $(I - G)\inv = \sum_{n=0}^\infty G^n$: 

\begin{align*}
\Delta_{i,j}'(\omega)  = \sum_{k,l}  \Delta_{ik}^0(\omega) \frac{\phi_k''}{2} \phi'_l J_{lj}(\omega ) \Delta_{lj}^0(\omega) \int d\omega'J_{k l}(\omega')\Delta^0_{kl}(\omega') J_{kl}(\omega - \omega') \Delta^0_{kl}(\omega - \omega')\\
 \int d\omega'J_{k l}(\omega')\Delta^0_{kl}(\omega') J_{kl}(\omega - \omega') \Delta^0_{kl}(\omega - \omega') \\=  \int d\omega'\frac{g_{kl}}{1 + 2\pi i \omega'\tau} \left(\sum_{n = 0}^\infty \left(\frac{1}{1 + 2\pi i \omega'\tau}\right)^n G^n\right)\frac{g_{kl}}{1 + 2\pi i (\omega-\omega')\tau} \left(\sum_{m = 0}^\infty \left(\frac{1}{1 + 2\pi i (\omega-\omega')\tau}\right)^m G^m \right)\\
g_{kl}^2 \sum_{m,n = 0}^\infty \left(G^n\right)_{kl}\left(G^m\right)_{kl} \int d\omega' \left(\frac{1}{1 + 2\pi i\omega' \tau}\right)^{n+1} \left(\frac{1}{1 + 2\pi i(\omega-\omega') \tau}\right)^{m+1} 
\end{align*}

Ok! Actually, we want to calculate the one-loop correction to the firing rate, rather than to the linear response. 

\begin{align*}
r_i(t) = \sum_{j,k} \int dt' dt'' \frac{f''(t')}{2}  \Delta_{ij}^0(t, t') \left( (J*\Delta^0)_{jk}(t', t'')\right)^2 r^0_k(t'')\\
\end{align*}
We assume things are stationary, so that $r_i$ does not depend on time and $ \Delta_{ij}^0(t, t')$ only depends on the time difference $s = t-t'$. With these assumptions
\begin{align*}
r_i = \sum_{j,k}\int ds ds'  \frac{f''}{2}  \Delta_{ij}^0(s) \left( (J*\Delta^0)_{jk}(s')\right)^2 r^0_k\\
\end{align*}
Now, we rewrite $\Delta^0_{ij}(s) = \int \frac{d\omega}{2\pi}e^{i\omega s}\Delta_{ij}^0(\omega)$ and  $(J*\Delta^0)_{jk}(s') = \int \frac{d\omega'}{2\pi} e^{iws'}(J(\omega)\Delta^0(\omega')_{jk}$.
\begin{align*}
r_i = \sum_{j,k}\int ds ds'  \frac{f''}{2} \left(\int \frac{d\omega}{2\pi}e^{i\omega s}\Delta_{ij}^0(\omega)\right)\left(\int \frac{d\omega'}{2\pi} e^{iws'}(J(\omega)\Delta^0(\omega')_{jk}\right)^2 r^0_k\\
r_i = \sum_{j,k}\int ds ds'  \frac{f''}{2} \left(\int \frac{d\omega}{2\pi}e^{i\omega s}\Delta_{ij}^0(\omega)\right)\int \frac{d\omega'}{2\pi} e^{iw's'}(J(\omega')\Delta^0(\omega')_{jk} \int \frac{d\omega''}{2\pi} e^{iw''s'}(J(\omega'')\Delta^0(\omega'')_{jk} r^0_k\\
\end{align*}
Now switching the order of integration and performing the integrals over $s$ and $s'$, we have 
\begin{align*}
r_i = \sum_{j,k} \int \frac{d\omega}{2\pi} \frac{d\omega'}{2\pi}\frac{d\omega''}{2\pi}  \frac{f''}{2}\delta(\omega) \delta(\omega'+\omega'') \Delta_{ij}^0(\omega) (J(\omega')\Delta^0(\omega') J(\omega'') \Delta^0(\omega'')r_k^0
\end{align*}
Now, integrating out the delta functions, we have 
\begin{align*}
r_i =  \frac{1}{4\pi^2}\sum_{j,k} \frac{f''}{2} r_k^0\Delta_{ij}^0(0)\int \frac{d\omega'}{2\pi} \left(J(\omega')\Delta^0(\omega')\right)_{jk} \left(J(-\omega') \Delta^0(-\omega')\right)_{jk}
\end{align*}
Now, filling in expressions for $J(\omega)$ and $\Delta_0(\omega)$, we have 
\begin{align*}
r_i &=  \frac{1}{4\pi^2}\sum_{j,k} \frac{f''}{2} r_k^0\Delta_{ij}^0(0)\int \frac{d\omega'}{2\pi} \left(\frac{1}{1 + 2\pi  i \omega}J(I - \frac{1}{1 + 2\pi  i \omega} J)\inv\right)_{jk}\left(\frac{1}{1 -2\pi  i \omega}J(I - \frac{1}{1 - 2\pi  i \omega} J)\inv\right)_{jk}\\
r_i &=  \frac{1}{4\pi^2}\sum_{j,k} \frac{f''}{2} r_k^0\Delta_{ij}^0(0)\int \frac{d\omega'}{2\pi} \left(\sum_{n =1}^\infty \left(\frac{1}{1 + 2\pi  i \omega}\right)^n J^n\right)_{jk}\left(\sum_{m =1}^\infty \left(\frac{1}{1 - 2\pi  i \omega}\right)^m J^m \right)_{jk}\\
r_i &=  \frac{1}{4\pi^2}\sum_{j,k} \frac{f''}{2} r_k^0\Delta_{ij}^0(0)\sum_{m, n} (J^{m})_{jk} (J^n)_{jk}\int \frac{d\omega'}{2\pi} \left(\frac{1}{1 + 2\pi  i \omega}\right)^n \left(\frac{1}{1 - 2\pi  i \omega}\right)^m 
\end{align*}

Now, our goal is for all $m$ and $n$ to evaluate the integral 

\begin{align*}
\int_{-\infty}^\infty d\omega (1 + 2\pi i \omega)^{-n} (1 - 2\pi i \omega)^{-m}
\end{align*} 
using the residue theorem. We rewrite it as 
\begin{align*}
 (-1)^n\left(\frac{-i}{2\pi}\right)^{n+m} \int_{-\infty}^\infty d\omega\left(\omega - \frac{i}{2\pi}\right)^{-n} \left(\omega - (- \frac{i}{2\pi})\right)^{-m}
\end{align*} 
Now, looping around the pole $z = \frac{i}{2\pi}$, letting $f(\omega) = \left(\omega - (- \frac{i}{2\pi})\right)^{-m}$,   by Cauchyâ€™s integral formula we have 



\begin{align*}
 \int_{-\infty}^\infty d\omega\left(\omega - \frac{i}{2\pi}\right)^{-n} \left(\omega - (- \frac{i}{2\pi})\right)^{-m} = \int_C \frac{f(\omega)}{(\omega -z)^n} = f^{(n-1)}(z) \frac{2\pi i}{(n-1)!} 
\end{align*}
where right now we're ignoring the limiting behavior and saying that $C$ is any contour that goes around this pole and not the other. 

Ok, now let's evaluate! 

\begin{align*}
f^{(n-1)}(z) = (-1)^{(n-1)} \frac{(m + n - 2)!}{(m -1)!}\left(z- \frac{-i}{2\pi}\right)^{(m +n -1)} =  (-1)^{(n-1)} \frac{(m + n - 2)!}{(m -1)!} \left(\frac{i}{\pi}\right)^{(m +n -1)}
\end{align*}
Thus 
\begin{align*}
f^{(n-1)}(z) \frac{2\pi i}{(n-1)!}  =   \frac{2\pi i}{(n-1)!} (-1)^{(n-1)} \frac{(m + n - 2)!}{(m -1)!} \left(\frac{i}{\pi}\right)^{(m +n -1)} = 2\pi i (-1)^{(n-1)}\binom{m + n -2}{m-1} \left(\frac{i}{\pi}\right)^{(m +n -1)}
\end{align*}
Now, completing the calculation, we have 

\begin{align*}
 (-1)^n\left(\frac{-i}{2\pi}\right)^{n+m} \int_{-\infty}^\infty d\omega\left(\omega - \frac{i}{2\pi}\right)^{-n} \left(\omega - (- \frac{i}{2\pi})\right)^{-m} \\
 = (-1)^{(n-1)} (-1)^n\left(\frac{-i}{2\pi}\right)^{n+m} 2\pi i \binom{m + n -2}{m-1} \left(\frac{i}{\pi}\right)^{(m +n -1)} \\
 =(-1)^{m  + n +1} (2\pi i) \binom{m +n -2}{m-1} \frac i {2^{m+n}\pi } 
\end{align*}

Going around the other pole, we get the same thing but with an extra factor of $-1$. This is exactly what we want for both methods of evaluating, since going around the other loop changes the winding number by a factor of $-1$. 

Now, substituting this back in to the formula for the correction to the firing rate, we have 
\begin{align*}
r_i' = \frac{1}{4\pi^3} \sum_{j,k} \frac {f''}{2} r_k^0 \Delta_{ij}^0 \sum_{m,n}^\infty (J^m)_{jk} (J^n)_{jk} \binom{m  + n -2}{m-1}\left(\frac{-1}{2}\right)^{m+n}\\
r_i' = \frac{1}{4\pi^3} \sum_{j,k} \frac {f''}{2} r_k^0 \Delta_{ij}^0 \sum_{m,n =1}^\infty \left(\frac{-1}{2}J\right)^n_{jk} \left(\frac{-1}{2}J\right)_{jk}^m \binom{m  + n -2}{m-1}\\
\end{align*}

Now, we split up the sum in terms of the value of $m+n = k$.

\begin{align*}
r_i' = \frac{1}{4\pi^3} \sum_{j,k} \frac {f''}{2} r_k^0 \Delta_{ij}^0 \sum_{k}^\infty \sum_{m+n = k} \left(\frac{-1}{2}J\right)^n_{jk} \left(\frac{-1}{2}J\right)_{jk}^m \binom{m  + n -2}{m-1}\\
\end{align*}

Now, note that if $J$ were a number, rather than a matrix, we would have $(1/2 J + 1/2 J)^{k } = \sum_{m = 0}^k \binom{k}{m} J^m J^n$. However, this doesn't work in our context, because of the distinction between matrix multiplication in forming the terms $J^n$ and $J^m$ elementwise multiplication in forming the term $\left(\frac{-1}{2}J\right)^n_{jk} \left(\frac{-1}{2}J\right)_{jk}^m$. 

Diagonalizing late in the game doesn't appear to fix this -- let's diagonalize early and see what happens: $$J(\omega) = \frac{1}{1 + 2\pi i \omega} PDP\inv = P \left(  \frac{1}{1 + 2\pi i \omega} D \right) P\inv$$ $$\Delta^0 = (I - \phi' J(\omega))\inv = (I - Q \frac{1}{1 + 2\pi i \omega} F Q\inv)\inv = Q ( I -  \frac{1}{1 + 2\pi i \omega} F)\inv Q\inv $$
\begin{align*}
r_i =  \frac{1}{4\pi^2}\sum_{j,k} \frac{f''}{2} r_k^0\Delta_{ij}^0(0)\int \frac{d\omega'}{2\pi} \left(J(\omega')\Delta^0(\omega')\right)_{jk} \left(J(-\omega') \Delta^0(-\omega')\right)_{jk}
\end{align*}


Let notation be as follows: 

\begin{align*}
\Delta_{ij}^0(s)&: \mbox{ Tree level propagator, time domain}\\
\hat\Delta_{ij}(\omega) &:  \mbox{ Tree level propagator, frequency domain}\\
J&: \mbox{ connectivity}\\
f&: \mbox{ nonlinear transfer function}\\
r'&: \mbox{ correction to firing rate}\\
r_0&: \mbox{ tree level firing rate}\\
h(t)&: \mbox{ filter (time domain)}\\
\hat h(\omega)&: \mbox{ filter (frequency domain)} 
\end{align*}

Now, our formula for the one-loop correction to the rate of the $i$-th neuron is 
\begin{align*}
r_i' =  \frac{1}{4\pi^2}\sum_{j,k} \frac{f''}{2} r_k^0\Delta_{ij}^0(0)\int \frac{d\omega'}{2\pi} \left(J(\omega')\Delta^0(\omega')\right)_{jk} \left(J(-\omega') \Delta^0(-\omega')\right)_{jk}
\end{align*}
Now, let $D_{f'}$ be the diagonal matrix with entires $f_i'$:
\begin{align*}
D_{f'} = \begin{pmatrix}
f_1' & \ldots & 0\\
\vdots & \ddots & \vdots\\
0 & \ldots  & f_n'
\end{pmatrix}
\end{align*}
Then the tree-level propagator in the frequency domain is 
$\hat \Delta_0 = (I - \hat h(\omega) D_f' J)\inv$. 
We can compute this by diagonalizing $\hat h(\omega) D_f' J$:  
Let 
\begin{align*}
D_f' J = V \begin{pmatrix}
\xi_1 & \ldots & 0\\
\vdots & \ddots & \vdots\\
0 & \ldots  & \xi_n
\end{pmatrix}V\inv\\
\hat \Delta_0(\omega) = V
\begin{pmatrix}
\frac{1}{1 - \hat h(\omega) \xi_i}& \ldots & 0\\
\vdots & \ddots & \vdots\\
0 & \ldots  & \frac{1}{1 - \hat h(\omega) \xi_n}&
\end{pmatrix}V\inv\\
\end{align*}
Then letting $D_{i}(\omega) = \frac{\hat h (\omega)}{1 - \hat h(\omega) \xi_i}$, we have 
\begin{align*}
(J(\omega) \hat \Delta_0(\omega))_{jk} = \sum_{\ell}(J V)_{jl} D_\ell(\omega) V_{\ell k }\inv
\end{align*}
Then the correction to the firing rates is 
\begin{align*}
r_i' =  \frac{1}{4\pi^2}\sum_{j,k} \frac{f''}{2} r_k^0\Delta_{ij}^0(0)\int \frac{d\omega'}{2\pi} \left(\sum_{\ell}(J V)_{jl} D_\ell(\omega) V_{\ell k }\inv\right) \left(\sum_{\ell}(J V)_{jl} D_\ell(-\omega) V_{\ell k }\inv\right)\\
r_i' =  \frac{1}{4\pi^2}\sum_{j,k, \ell, m} \frac{f''}{2} r_k^0\Delta_{ij}^0(0) (J V)_{j\ell} V_{\ell k }\inv (J V)_{jm}  V_{m k }\inv \int \frac{d\omega'}{2\pi} D_\ell(\omega)D_m(-\omega)
\end{align*}
Thus, the integral we need to do over $\omega$ is 
\begin{align*}
\int \frac{d\omega'}{2\pi} D_\ell(\omega)D_m(-\omega)
\end{align*}
We can evaluate this using the residue theorem. For exponential decaying synapses, we have 
\begin{align*}
\hat h (\omega) = \frac{1}{1 +2\pi i \omega \tau}\\
 D_\ell(\omega) = \frac{1}{1 + 2\pi i \omega \tau - \xi_\ell}\\
 \int \frac{d\omega'}{2\pi} D_\ell(\omega)D_m(-\omega)
= 
 \int \frac{d\omega'}{2\pi} \frac{1}{1 + 2\pi i \omega \tau - \xi_\ell} \frac{1}{1 - 2\pi i \omega \tau - \xi_m}
\end{align*}
This has two poles, 
$\omega_\ell = \frac{i (1 - \xi_\ell)}{2\pi \tau}$ and $\omega_m = \frac{-i (1 - \xi_m)}{2\pi \tau}$. Note by our assumption that the dynamics are stable, we have $\xi_m, \xi_\ell < 1$. Thus, only $\omega_\ell$ is in the upper half plane. Thus, the value of the integral $\int \frac{d\omega}{2\pi} \frac{1}{1 + 2\pi i \omega \tau - \xi_\ell} \frac{1}{1 - 2\pi i \omega \tau - \xi_m}$ is the residue at the pole $\omega_m$. We have
\begin{align*}
\int \frac{d\omega}{2\pi} \frac{1}{1 + 2\pi i \omega \tau - \xi_\ell} \frac{1}{1 - 2\pi i \omega \tau - \xi_m} = \frac{1}{2 - \xi_\ell - \xi_m}
\end{align*}
(Evaluating by looping around the other pole gives the same result, as we'd expect.) 

Thus we have 

\begin{align*}
r_i' = \sum_{j, k} (1/2\pi)^2 \frac{f_j''}{2} r_k^0 \hat \Delta_{ij}(0) \sum_{\ell, m} (JV)_{j\ell} V_{\ell k}\inv(JV)_{jm} V\inv_{mk} \left(\frac 1 { 2  - \xi_\ell - \xi_m}\right)
\end{align*}



\bibliography{memory}
\bibliographystyle{plain}
\end{document}

